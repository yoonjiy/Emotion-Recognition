{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "import cv2\n",
    "from models import sample"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 48, 48, 1)]       0         \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 46, 46, 32)        320       \n",
      "                                                                 \n",
      " batch_normalization_10 (Bat  (None, 46, 46, 32)       128       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_10 (Activation)  (None, 46, 46, 32)        0         \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 46, 46, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization_11 (Bat  (None, 46, 46, 64)       256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_11 (Activation)  (None, 46, 46, 64)        0         \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPooling  (None, 23, 23, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_12 (Conv2D)          (None, 21, 21, 64)        36928     \n",
      "                                                                 \n",
      " batch_normalization_12 (Bat  (None, 21, 21, 64)       256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_12 (Activation)  (None, 21, 21, 64)        0         \n",
      "                                                                 \n",
      " conv2d_13 (Conv2D)          (None, 21, 21, 128)       73856     \n",
      "                                                                 \n",
      " batch_normalization_13 (Bat  (None, 21, 21, 128)      512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_13 (Activation)  (None, 21, 21, 128)       0         \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPooling  (None, 10, 10, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_14 (Conv2D)          (None, 8, 8, 128)         147584    \n",
      "                                                                 \n",
      " batch_normalization_14 (Bat  (None, 8, 8, 128)        512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_14 (Activation)  (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPooling  (None, 4, 4, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 200)               409800    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 200)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 7)                 1407      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 690,055\n",
      "Trainable params: 689,223\n",
      "Non-trainable params: 832\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = sample.emotion_recognition((48,48,1))\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "emotion_classifier = model.load_weights('../trained_models/emotion_weights_30.hdf5')\n",
    "face_detection = cv2.CascadeClassifier('../trained_models/haarcascade_frontalface_default.xml')\n",
    "label_dict = {0 : 'Angry', 1 : 'Disgust', 2 : 'Fear', 3 : 'Happiness', 4 : 'Sad', 5 : 'Surprise', 6 : 'Neutral'}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[1;32mIn [11]\u001B[0m, in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      5\u001B[0m _,cap_image \u001B[38;5;241m=\u001B[39m cap\u001B[38;5;241m.\u001B[39mread()\n\u001B[0;32m      6\u001B[0m cap_img_gray \u001B[38;5;241m=\u001B[39m cv2\u001B[38;5;241m.\u001B[39mcvtColor(cap_image, cv2\u001B[38;5;241m.\u001B[39mCOLOR_BGR2GRAY)\n\u001B[1;32m----> 7\u001B[0m faces \u001B[38;5;241m=\u001B[39m \u001B[43mface_detection\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdetectMultiScale\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcap_img_gray\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1.3\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m (x,y,w,h) \u001B[38;5;129;01min\u001B[39;00m faces:\n\u001B[0;32m     11\u001B[0m   cv2\u001B[38;5;241m.\u001B[39mrectangle(cap_image, (x,y), (x\u001B[38;5;241m+\u001B[39mw,y\u001B[38;5;241m+\u001B[39mh),(\u001B[38;5;241m255\u001B[39m,\u001B[38;5;241m0\u001B[39m,\u001B[38;5;241m0\u001B[39m),\u001B[38;5;241m2\u001B[39m)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "\n",
    " _,cap_image = cap.read()\n",
    " cap_img_gray = cv2.cvtColor(cap_image, cv2.COLOR_BGR2GRAY)\n",
    " faces = face_detection.detectMultiScale(cap_img_gray, 1.3, 5)\n",
    "\n",
    " for (x,y,w,h) in faces:\n",
    "\n",
    "   cv2.rectangle(cap_image, (x,y), (x+w,y+h),(255,0,0),2)\n",
    "   roi_gray = cap_img_gray[y:y+h, x:x+w]\n",
    "   roi_gray = cv2.resize(roi_gray, (48,48))\n",
    "   img_pixels = image.img_to_array(roi_gray)\n",
    "   img_pixels = np.expand_dims(img_pixels, axis=0)\n",
    "\n",
    "   predictions = model.predict(img_pixels)\n",
    "   emotion_label = np.argmax(predictions)\n",
    "\n",
    "   emotion_prediction = label_dict[emotion_label]\n",
    "\n",
    "   cv2.putText(cap_image, emotion_prediction, (int(x), int(y)), cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0,0,0), 1 )\n",
    "\n",
    "   resize_image = cv2.resize(cap_image, (1000,700))\n",
    "   cv2.imshow('Emotion',resize_image)\n",
    "\n",
    "   if cv2.waitKey(1) == ord('q'):\n",
    "    break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}